{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that instantiates an object that creates and solves linear equation for the given gridworld problem\n",
    "class LinearEqnSolver:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.a = np.zeros((25,25),dtype=\"float32\") # Stores coefficients of variables\n",
    "        self.b = np.array([0.5,-10,0.25,-5,0.5,0.25,0,0,0,0.25,0.25,0,0,0,0.25,0.25,0,0,0,0.25,0.5,0.25,0.25,0.25,0.5]) # Stores constants\n",
    "        self.populate_matrix() # Builds coefficient matrix\n",
    "\n",
    "    # function updates values in coefficient matrix\n",
    "    def add(self,current,states,vals):\n",
    "        for i in range(len(states)):\n",
    "            row = current - 1\n",
    "            col = states[i]-1\n",
    "            self.a[row,col] = vals[i]\n",
    "        \n",
    "    # Populates values of coefficient matrix from the linear equations\n",
    "    def populate_matrix(self):\n",
    "        self.add(1,[1,2,6],[-0.55,0.225,0.225])\n",
    "        self.add(2,[2,22],[-1,0.9])\n",
    "        self.add(3,[2,3,4,8],[0.225,-0.775,0.225,0.225])\n",
    "        self.add(4,[4,14],[-1,0.9])\n",
    "        self.add(5,[4,5,10],[0.225,-0.55,0.225])\n",
    "\n",
    "        self.add(6,[1,6,7,11],[0.225,-0.775,0.225,0.225])\n",
    "        self.add(7,[2,6,7,8,12],[0.225,0.225,-1,0.225,0.225])\n",
    "        self.add(8,[3,7,8,9,13],[0.225,0.225,-1,0.225,0.225])\n",
    "        self.add(9,[4,8,9,10,14],[0.225,0.225,-1,0.225,0.225])\n",
    "        self.add(10,[5,10,9,15],[0.225,-0.775,0.225,0.225])\n",
    "\n",
    "        self.add(11,[6,11,12,16],[0.225,-0.775,0.225,0.225])\n",
    "        self.add(12,[7,11,12,13,17],[0.225,0.225,-1,0.225,0.225])\n",
    "        self.add(13,[8,12,13,14,18],[0.225,0.225,-1,0.225,0.225])\n",
    "        self.add(14,[9,13,14,15,19],[0.225,0.225,-1,0.225,0.225])\n",
    "        self.add(15,[10,15,14,20],[0.225,-0.775,0.225,0.225])\n",
    "\n",
    "        self.add(16,[11,16,17,21],[0.225,-0.775,0.225,0.225])\n",
    "        self.add(17,[12,16,17,18,22],[0.225,0.225,-1,0.225,0.225])\n",
    "        self.add(18,[13,17,18,19,23],[0.225,0.225,-1,0.225,0.225])\n",
    "        self.add(19,[14,18,19,20,24],[0.225,0.225,-1,0.225,0.225])\n",
    "        self.add(20,[15,20,19,25],[0.225,-0.775,0.225,0.225])\n",
    "\n",
    "        self.add(21,[16,21,22],[0.225,-0.55,0.225])\n",
    "        self.add(22,[17,22,21,23],[0.225,-0.775,0.225,0.225])\n",
    "        self.add(23,[18,23,22,24],[0.225,-0.775,0.225,0.225])\n",
    "        self.add(24,[19,24,23,25],[0.225,-0.775,0.225,0.225])\n",
    "        self.add(25,[20,25,24],[0.225,-0.55,0.225])\n",
    "\n",
    "\n",
    "    # Solves bellman equations for the gridworld by solving the linear equations\n",
    "    def solve_bellman_eqns_fig3_2(self):\n",
    "        x = np.linalg.solve(self.a, self.b)\n",
    "        x = np.reshape(x,(5,5))\n",
    "        print(np.round(x, 1) )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gridsize = 5 # Grid size\n",
    "        self.prob_actions = 0.25 # Probability of picking an action in a state\n",
    "        self.discount = 0.9 # Discount factor for return calculation\n",
    "        self.actions = [[0,1],[1,0],[0,-1],[-1,0]] # Right,Down,Left,Up\n",
    "        self.actions_meanings = [\"→\",\"↓\",\"←\",\"↑\"]\n",
    "        self.v = np.zeros((self.gridsize,self.gridsize)) # estimated returns of each state (positions on the grid)\n",
    "        self.actions_map = {}\n",
    "        for i in range(len(self.actions)):\n",
    "            self.actions_map[str(self.actions[i])] = self.actions_meanings[i]\n",
    "            \n",
    "        self.optimal_policy = [[ \"↓ ↑ ← →\" for i in range(self.gridsize)] for j in range(self.gridsize)]\n",
    "            \n",
    "            \n",
    "        \n",
    "    def create_optimal_policy(self,env):\n",
    "        for i in range(self.gridsize):\n",
    "            for j in range(self.gridsize): \n",
    "                # Stores the maximum action value function of all actions in given state\n",
    "                q = float('-inf')\n",
    "                # Stores Optimal action(s) for given state\n",
    "                a = \"\"\n",
    "\n",
    "                # Iterating over all actions in a state\n",
    "                for action in self.actions:\n",
    "                    # performing given action and obtaining next state and reward\n",
    "                    reward,next_x,next_y = env.perform_action([i,j],action)\n",
    "\n",
    "                    # checking whether action value of given state is greater/equal to current max\n",
    "                    if q < reward+self.discount*self.v[next_x,next_y]:\n",
    "                        q = reward+self.discount*self.v[next_x,next_y]\n",
    "                        a = self.actions_map[str(action)]\n",
    "                    elif q == reward+self.discount*self.v[next_x,next_y]:\n",
    "                        a = a+\" \"+self.actions_map[str(action)]\n",
    "                    \n",
    "                # Updating optimal policy of given state\n",
    "                self.optimal_policy[i][j] = \" \"*int((7-len(a))/2) + a + \" \"*((7-len(a) - int((7-len(a))/2)))\n",
    "                \n",
    "        print('\\n',np.array(self.optimal_policy))\n",
    "\n",
    "    def fig3_2(self,env):\n",
    "        # Iterating until convergence\n",
    "        while True: \n",
    "            # 2d array to store updated states\n",
    "            next_v = np.zeros((self.gridsize,self.gridsize)) \n",
    "            \n",
    "            for i in range(self.gridsize):\n",
    "                for j in range(self.gridsize): \n",
    "                    # Iterating over all actions in a state\n",
    "                    for action in self.actions:\n",
    "                        # performing given action and obtaining next state and reward\n",
    "                        reward,next_x,next_y = env.perform_action([i,j],action)\n",
    "                        # updating value function of given state using bellman's equation\n",
    "                        next_v[i,j] += self.prob_actions*(reward+self.discount*self.v[next_x,next_y]) \n",
    "            # Calculating absolute change over iteration\n",
    "            total_change = float(np.sum(np.absolute(next_v-self.v))) \n",
    "            # if change sufficiently small as compared to value function --> convergence\n",
    "            if total_change < float(np.sum(self.v))*(10**-6): \n",
    "                self.v = next_v\n",
    "                # rounding to 1 decimal place\n",
    "                self.v = np.round(self.v, 1) \n",
    "                print(self.v)\n",
    "                break\n",
    "            self.v = next_v\n",
    "            \n",
    "    def fig3_5(self,env):\n",
    "        # Iterating until convergence\n",
    "        while True: \n",
    "            # 2d array to store updated states\n",
    "            next_v = np.zeros((self.gridsize,self.gridsize)) \n",
    "            for i in range(self.gridsize):\n",
    "                for j in range(self.gridsize): \n",
    "                    # Stores the maximum action value function of all actions in given state\n",
    "                    q = float('-inf')\n",
    "                    \n",
    "                    # Iterating over all actions in a state\n",
    "                    for action in self.actions:\n",
    "                        # performing given action and obtaining next state and reward\n",
    "                        reward,next_x,next_y = env.perform_action([i,j],action)\n",
    "                        # updating max action value in given state\n",
    "                        q = max(q,reward+self.discount*self.v[next_x,next_y])\n",
    "                    \n",
    "                    # Assigning max action value for state as updated state value\n",
    "                    next_v[i,j] = q\n",
    "                         \n",
    "            # Calculating absolute change over iteration\n",
    "            total_change = float(np.sum(np.absolute(next_v-self.v))) \n",
    "            # if change sufficiently small as compared to value function --> convergence\n",
    "            if total_change < float(np.sum(self.v))*(10**-6): \n",
    "                self.v = next_v\n",
    "                # rounding to 1 decimal place\n",
    "                self.v = np.round(self.v, 1) \n",
    "                print(self.v)\n",
    "                break\n",
    "            self.v = next_v\n",
    "        self.create_optimal_policy(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gridsize = 5 # Grid size\n",
    "        self.A,self.B = [0,1],[0,3] # Special states\n",
    "        self.A_dest,self.B_dest = [4,1],[2,3] # Destination from special states\n",
    "        self.actions = [[0,1],[1,0],[0,-1],[-1,0]]  # Right,Down,Left,Up\n",
    "        self.reward_A,self.reward_B = 10, 5 # Rewards from special states\n",
    "        self.penalty_outside = -1 # Penalty of exiting grid\n",
    "        self.reward_other_actions = 0 # Reward for any action onto non special, valid state\n",
    "        \n",
    "    def perform_action(self, state, action):\n",
    "        # Checking for special states\n",
    "        if state == self.A: \n",
    "            return tuple([self.reward_A]+self.A_dest)\n",
    "        if state == self.B:\n",
    "            return tuple([self.reward_B]+self.B_dest)\n",
    "        \n",
    "        # Calculating next state\n",
    "        next_state = [state[0]+action[0],state[1]+action[1]]\n",
    "        # Checking for out of grid state\n",
    "        if next_state[0]<0 or next_state[1]<0 or next_state[0]>=self.gridsize or next_state[1]>=self.gridsize:\n",
    "            return tuple([self.penalty_outside]+state)\n",
    "        # Returning next state and reward\n",
    "        return tuple([self.reward_other_actions]+next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.3  8.8  4.4  5.3  1.5]\n",
      " [ 1.5  3.   2.3  1.9  0.5]\n",
      " [ 0.1  0.7  0.7  0.4 -0.4]\n",
      " [-1.  -0.4 -0.4 -0.6 -1.2]\n",
      " [-1.9 -1.3 -1.2 -1.4 -2. ]]\n"
     ]
    }
   ],
   "source": [
    "# Generating Figure 3.2 by solving system of linear equations (bellman state value function equations of each state)\n",
    "solver = LinearEqnSolver()\n",
    "solver.solve_bellman_eqns_fig3_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.3  8.8  4.4  5.3  1.5]\n",
      " [ 1.5  3.   2.3  1.9  0.5]\n",
      " [ 0.1  0.7  0.7  0.4 -0.4]\n",
      " [-1.  -0.4 -0.4 -0.6 -1.2]\n",
      " [-1.9 -1.3 -1.2 -1.4 -2. ]]\n"
     ]
    }
   ],
   "source": [
    "# Setup to generate fig 3.2 using dynamic programming and policy evaluation\n",
    "agent = Agent()\n",
    "env = Environment()\n",
    "agent.fig3_2(env)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22.  24.4 22.  19.4 17.5]\n",
      " [19.8 22.  19.8 17.8 16. ]\n",
      " [17.8 19.8 17.8 16.  14.4]\n",
      " [16.  17.8 16.  14.4 13. ]\n",
      " [14.4 16.  14.4 13.  11.7]]\n",
      "\n",
      " [['   →   ' '→ ↓ ← ↑' '   ←   ' '→ ↓ ← ↑' '   ←   ']\n",
      " ['  → ↑  ' '   ↑   ' '  ← ↑  ' '   ←   ' '   ←   ']\n",
      " ['  → ↑  ' '   ↑   ' '  ← ↑  ' '  ← ↑  ' '  ← ↑  ']\n",
      " ['  → ↑  ' '   ↑   ' '  ← ↑  ' '  ← ↑  ' '  ← ↑  ']\n",
      " ['  → ↑  ' '   ↑   ' '  ← ↑  ' '  ← ↑  ' '  ← ↑  ']]\n"
     ]
    }
   ],
   "source": [
    "# Setup to generate fig 3.5 using dynamic programming\n",
    "agent2 = Agent()\n",
    "env2 = Environment()\n",
    "agent2.fig3_5(env2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
