{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gridsize = 4 # Grid size\n",
    "        self.policy = np.ones((self.gridsize,self.gridsize,4))*0.25 # Policy\n",
    "        self.discount = 1 # Discount factor for return calculation\n",
    "        self.actions = [[0,1],[1,0],[0,-1],[-1,0]] # Right,Down,Left,Up\n",
    "        self.actions_meanings = [\"→\",\"↓\",\"←\",\"↑\"]\n",
    "        self.v = np.zeros((self.gridsize,self.gridsize)) # estimated returns of each state (positions on the grid)\n",
    "        self.actions_map = {}\n",
    "        for i in range(len(self.actions)):\n",
    "            self.actions_map[str(self.actions[i])] = self.actions_meanings[i]\n",
    "            \n",
    "        self.optimal_policy = [[ \"↓ ↑ ← →\" for i in range(self.gridsize)] for j in range(self.gridsize)]\n",
    "        \n",
    "    def display_policy(self):\n",
    "        for i in range(self.gridsize):\n",
    "            for j in range(self.gridsize): \n",
    "                action_seq = \"\"\n",
    "                for action_ind in range(len(self.actions)): \n",
    "                    if self.policy[i,j,action_ind] > 0:\n",
    "                        action_seq = action_seq+self.actions_map[str(self.actions[action_ind])]+\" \"\n",
    "                        \n",
    "                self.optimal_policy[i][j] = \" \"*int((8-len(action_seq))/2) + action_seq + \" \"*((8-len(action_seq) - int((8-len(action_seq))/2)))\n",
    "        \n",
    "        self.optimal_policy[0][0] = \" \"*7\n",
    "        self.optimal_policy[self.gridsize-1][self.gridsize-1] = \" \"*7\n",
    "        \n",
    "        print('\\n',np.array(self.optimal_policy))       \n",
    "        \n",
    "    def value_iteration(self,env):\n",
    "            \n",
    "        # Threshold to check for convergence\n",
    "        threshold = 10**-4  \n",
    "        # Iterating until convergence\n",
    "        while True:  \n",
    "            # Absolute difference between consecutive v(s) estimates\n",
    "            cur_diff = 0\n",
    "            for i in range(self.gridsize):\n",
    "                for j in range(self.gridsize): \n",
    "                    # Next value for current state\n",
    "                    next_v = -1000 \n",
    "                    # Iterating over all actions in a state\n",
    "                    for act_ind in range(len(self.actions)):\n",
    "                        # performing given action and obtaining next state and reward\n",
    "                        reward,next_x,next_y = env.perform_action([i,j],self.actions[act_ind])\n",
    "                        # updating value function of given state using bellman's equation\n",
    "                        next_v = max(next_v,reward+self.discount*self.v[next_x,next_y]) \n",
    "                    cur_diff = max(cur_diff,abs(next_v-self.v[i,j]))\n",
    "                    self.v[i,j] = next_v\n",
    "                    \n",
    "            print(\"\\n\")\n",
    "            print(np.round(self.v, 1))\n",
    "                    \n",
    "            # If difference sufficiently small --> convergence\n",
    "            if cur_diff < threshold: \n",
    "                # rounding to 1 decimal place\n",
    "                # self.v = np.round(self.v, 1) \n",
    "                break\n",
    "\n",
    "\n",
    "        ###### Optimal Policy Selection ######\n",
    "        for i in range(self.gridsize):\n",
    "            for j in range(self.gridsize):\n",
    "                # Stores the maximum action value function of all actions in given state\n",
    "                q = float('-inf')\n",
    "                # Stores Optimal action(s) for given state\n",
    "                a = []\n",
    "                # Iterating over all actions in a state\n",
    "                for act_ind in range(len(self.actions)):\n",
    "                    # performing given action and obtaining next state and reward\n",
    "                    reward,next_x,next_y = env.perform_action([i,j],self.actions[act_ind])\n",
    "\n",
    "                    # checking whether action value of given state is greater/equal to current max\n",
    "                    if q < reward+self.discount*self.v[next_x,next_y]:\n",
    "                        q = reward+self.discount*self.v[next_x,next_y]\n",
    "                        a = [act_ind]\n",
    "                    elif q == reward+self.discount*self.v[next_x,next_y]:\n",
    "                        a.append(act_ind)\n",
    "\n",
    "                # Savin old state to check stability later\n",
    "                old_state_policy = self.policy[i,j,:] > 0\n",
    "\n",
    "                # Computing new policy (stocastic)\n",
    "                for action_ind in range(4):\n",
    "                    if action_ind not in a: \n",
    "                        self.policy[i,j,action_ind] = 0\n",
    "                    else:\n",
    "                        self.policy[i,j,action_ind] = 1./len(a)\n",
    "\n",
    "        print(\"\\n Optimal Policy and Value Function\")\n",
    "        print('\\n',np.round(self.v, 1) )\n",
    "        self.display_policy()\n",
    "   \n",
    "            \n",
    "\n",
    "    def policy_iteration(self,env):\n",
    "        \n",
    "        # Run till convergence achieved\n",
    "        while True:\n",
    "            \n",
    "            ###### Policy Evaluation ######\n",
    "            # Threshold to check for convergence\n",
    "            threshold = 10**-4  \n",
    "            cur_diff = 0\n",
    "            # Iterating until convergence\n",
    "            while True:  \n",
    "                # Absolute difference between consecutive v(s) estimates\n",
    "                cur_diff = 0\n",
    "                for i in range(self.gridsize):\n",
    "                    for j in range(self.gridsize): \n",
    "                        # Next value for current state\n",
    "                        next_v = 0\n",
    "                        # Iterating over all actions in a state\n",
    "                        for act_ind in range(len(self.actions)):\n",
    "                            # performing given action and obtaining next state and reward\n",
    "                            reward,next_x,next_y = env.perform_action([i,j],self.actions[act_ind])\n",
    "                            # updating value function of given state using bellman's equation\n",
    "                            next_v += self.policy[i,j,act_ind]*(reward+self.discount*self.v[next_x,next_y]) \n",
    "#                             if i==0 and j==1:\n",
    "#                                 print(\"action = \",self.actions_map[str(self.actions[act_ind])],\"  Next v = \",next_v,\"  Reward = \",reward,\"  Val func = \", self.v[next_x,next_y],next_x,next_y)\n",
    "                        cur_diff = max(cur_diff,abs(next_v-self.v[i,j]))\n",
    "                        self.v[i,j] = next_v\n",
    "#                 print(\"\\n\")\n",
    "                        \n",
    "#                 print('\\n',np.round(self.v, 1) )\n",
    "                    \n",
    "                # If difference sufficiently small --> convergence\n",
    "                if cur_diff < threshold: \n",
    "                    # rounding to 1 decimal place\n",
    "                    # self.v = np.round(self.v, 1) \n",
    "                    break\n",
    "\n",
    "            ###### Policy Improvement ######\n",
    "            policy_stable = True\n",
    "            for i in range(self.gridsize):\n",
    "                for j in range(self.gridsize):\n",
    "                    # Stores the maximum action value function of all actions in given state\n",
    "                    q = float('-inf')\n",
    "                    # Stores Optimal action(s) for given state\n",
    "                    a = []\n",
    "                    # Iterating over all actions in a state\n",
    "                    for act_ind in range(len(self.actions)):\n",
    "                        # performing given action and obtaining next state and reward\n",
    "                        reward,next_x,next_y = env.perform_action([i,j],self.actions[act_ind])\n",
    "\n",
    "                        # checking whether action value of given state is greater/equal to current max\n",
    "                        if q < reward+self.discount*self.v[next_x,next_y]:\n",
    "                            q = reward+self.discount*self.v[next_x,next_y]\n",
    "                            a = [act_ind]\n",
    "                        elif q == reward+self.discount*self.v[next_x,next_y]:\n",
    "                            a.append(act_ind)\n",
    "\n",
    "                    # Savin old state to check stability later\n",
    "                    old_state_policy = self.policy[i,j,:] > 0\n",
    "\n",
    "                    # Computing new policy (stocastic)\n",
    "                    for action_ind in range(4):\n",
    "                        if action_ind not in a: \n",
    "                            self.policy[i,j,action_ind] = 0\n",
    "                        else:\n",
    "                            self.policy[i,j,action_ind] = 1./len(a)\n",
    "                            \n",
    "                    new_state_policy = self.policy[i,j,:] > 0\n",
    "                    \n",
    "                    # Checking if policy remained stable in given state\n",
    "                    if np.any(np.logical_xor(old_state_policy,new_state_policy) == True) :\n",
    "                        policy_stable = False\n",
    "                        \n",
    "            print('\\n',np.round(self.v, 1) )\n",
    "            self.display_policy()\n",
    "                        \n",
    "            \n",
    "            # If policy stable ---> optimal policy achieved \n",
    "            if policy_stable: \n",
    "                break\n",
    "            \n",
    "        print(\"\\n\\n Dispalying Optimal Policy and Value Function\")\n",
    "        print('\\n',np.round(self.v, 2) )\n",
    "        self.display_policy()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.gridsize = 4 # Grid size\n",
    "        self.terminal_states = [[0,0],[self.gridsize - 1,self.gridsize - 1]] # Terminal states\n",
    "        self.terminal_rewards = 0\n",
    "        self.actions = [[0,1],[1,0],[0,-1],[-1,0]]  # Right,Down,Left,Up\n",
    "        self.non_term_reward = -1\n",
    "        \n",
    "    def perform_action(self, state, action):\n",
    "        # Checking for special states\n",
    "        if state in self.terminal_states: \n",
    "            return tuple([self.terminal_rewards]+state)\n",
    "        \n",
    "        # Calculating next state\n",
    "        next_state = [state[0]+action[0],state[1]+action[1]]\n",
    "        \n",
    "        # Checking for out of grid state\n",
    "        if next_state[0]<0 or next_state[1]<0 or next_state[0]>=self.gridsize or next_state[1]>=self.gridsize:\n",
    "            return tuple([self.non_term_reward]+state)\n",
    "        \n",
    "        # Returning next state and reward\n",
    "        return tuple([self.non_term_reward]+next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "\n",
      " [['       ' '   ←    ' '   ←    ' '   ←    ']\n",
      " ['   ↑    ' '  ← ↑   ' '   ←    ' '   ↓    ']\n",
      " ['   ↑    ' '   ↑    ' '   ↓    ' '   ↓    ']\n",
      " ['   ↑    ' '   →    ' '   →    ' '       ']]\n",
      "\n",
      " [[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      " [['       ' '   ←    ' '   ←    ' '  ↓ ←   ']\n",
      " ['   ↑    ' '  ← ↑   ' '→ ↓ ← ↑ ' '   ↓    ']\n",
      " ['   ↑    ' '→ ↓ ← ↑ ' '  → ↓   ' '   ↓    ']\n",
      " ['  → ↑   ' '   →    ' '   →    ' '       ']]\n",
      "\n",
      " [[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      " [['       ' '   ←    ' '   ←    ' '  ↓ ←   ']\n",
      " ['   ↑    ' '  ← ↑   ' '→ ↓ ← ↑ ' '   ↓    ']\n",
      " ['   ↑    ' '→ ↓ ← ↑ ' '  → ↓   ' '   ↓    ']\n",
      " ['  → ↑   ' '   →    ' '   →    ' '       ']]\n",
      "\n",
      "\n",
      " Dispalying Optimal Policy and Value Function\n",
      "\n",
      " [[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      " [['       ' '   ←    ' '   ←    ' '  ↓ ←   ']\n",
      " ['   ↑    ' '  ← ↑   ' '→ ↓ ← ↑ ' '   ↓    ']\n",
      " ['   ↑    ' '→ ↓ ← ↑ ' '  → ↓   ' '   ↓    ']\n",
      " ['  → ↑   ' '   →    ' '   →    ' '       ']]\n"
     ]
    }
   ],
   "source": [
    "# Setup to generate fig 4.1 using policy iteration\n",
    "agent = Agent()\n",
    "env = Environment()\n",
    "agent.policy_iteration(env)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -1.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -1. -1.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1.  0.  0.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1.  0.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -2.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -2.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -2.]\n",
      " [-1. -2. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -2.]\n",
      " [-1. -2. -2. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -2.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -2.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-2. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -2.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-2. -2. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -2.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-2. -2. -2. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -2.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-2. -2. -2. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -2.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-2. -2. -2. -1.]\n",
      " [-2. -1. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -2.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-2. -2. -2. -1.]\n",
      " [-2. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -2.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-2. -2. -2. -1.]\n",
      " [-2. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -2.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-2. -2. -2. -1.]\n",
      " [-2. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -2.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-2. -2. -2. -1.]\n",
      " [-2. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -2.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-2. -2. -2. -1.]\n",
      " [-2. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -2.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-2. -2. -2. -1.]\n",
      " [-2. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-2. -2. -2. -1.]\n",
      " [-2. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-2. -2. -2. -1.]\n",
      " [-2. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-2. -2. -2. -1.]\n",
      " [-2. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -2. -2. -1.]\n",
      " [-2. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -2. -2. -1.]\n",
      " [-2. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -2. -2. -1.]\n",
      " [-2. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-2. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-2. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-2. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      "\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      " Optimal Policy and Value Function\n",
      "\n",
      " [[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n",
      " [['       ' '   ←    ' '   ←    ' '  ↓ ←   ']\n",
      " ['   ↑    ' '  ← ↑   ' '→ ↓ ← ↑ ' '   ↓    ']\n",
      " ['   ↑    ' '→ ↓ ← ↑ ' '  → ↓   ' '   ↓    ']\n",
      " ['  → ↑   ' '   →    ' '   →    ' '       ']]\n"
     ]
    }
   ],
   "source": [
    "# Setup to generate fig 4.1 using value iteration\n",
    "agent = Agent()\n",
    "env = Environment()\n",
    "agent.value_iteration(env)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
